{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependancies\n",
    "# Don't run this if you installed via requirements.txt\n",
    "# %pip install gymnasium numpy tensorflow matplotlib pygame shapely pydot graphviz\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium Cope matplotlib pygame shapely pydot graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 11:37:22.867486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-27 11:37:22.867516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-27 11:37:22.867539: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-27 11:37:22.872389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 11:37:23.431592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from SquareEnv2 import SquareEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space -> 33\n",
      "Size of Action Space -> 33\n",
      "Max Value of an Action -> [0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05\n",
      " 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05\n",
      " 0.05 0.03 0.05 0.05 0.03]\n",
      "Min Value of an Action -> [-0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03\n",
      " -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03\n",
      " -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03]\n"
     ]
    }
   ],
   "source": [
    "# Configuration & Enviorment\n",
    "\n",
    "# I don't need it to be determinisitic\n",
    "# seed = 42\n",
    "\n",
    "# Set up the enviorment\n",
    "# After several hours messing with useless nested folders and __init__.py files, I gave up and\n",
    "# just did it the way that makes sense instead of the way that appeals to the standard\n",
    "# So instead of having useless nested directories, I just import directly from the file. I'm not\n",
    "# intending to publish the enviorment by itself.\n",
    "env = SquareEnv(\n",
    "    N=11,\n",
    "    render_mode=\"pygame\",\n",
    "    shift_rate=.05,\n",
    "    rot_rate=.03,\n",
    "    max_steps=300,\n",
    "    bound_method='clip',\n",
    "    flatten=True,\n",
    "    boundary=2,\n",
    "    max_overlap=.1\n",
    ")\n",
    "\n",
    "num_states = np.product(env.observation_space.shape)\n",
    "print(f\"Size of State Space -> {num_states}\")\n",
    "num_actions = np.product(env.action_space.shape)\n",
    "print(f\"Size of Action Space -> {num_actions}\")\n",
    "\n",
    "upper_bound = env.action_space.high\n",
    "lower_bound = env.action_space.low\n",
    "\n",
    "print(f\"Max Value of an Action -> {upper_bound}\")\n",
    "print(f\"Min Value of an Action -> {lower_bound}\")\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, state, action, reward, next_state):\n",
    "        # Set index to zero if buffer_capacity is exceeded, replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = state\n",
    "        self.action_buffer[index] = action\n",
    "        self.reward_buffer[index] = reward\n",
    "        self.next_state_buffer[index] = next_state\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch):\n",
    "        print('update called')\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # What we think we should do\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            # How good we think that is (the target Q-Network)\n",
    "            target_evaluation = target_critic([next_state_batch, target_actions], training=True)\n",
    "            y = reward_batch + gamma * target_evaluation\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            # Get the average amount we were off by in predicting how well we would do, squared, and that's the loss\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        # print('critic gradients:')\n",
    "        # print(critic_grad)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "# This updates target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor & Critic Models\n",
    "# Tells a later cell to reset the weights (since we might have changed how we initialize them in this cell)\n",
    "INIT_WEIGHTS = False\n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights\n",
    "    # last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n",
    "    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n",
    "    # outputs = layers.Dense(num_actions, activation='softsign', kernel_initializer=last_init)(out)\n",
    "    outputs = layers.Dense(num_actions, activation='softsign')(out)\n",
    "\n",
    "    # This is rescale the output from between -1 - 1 to the appropriate action space scale\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "# Exactly the same, but using Sequential, which I understand better\n",
    "def get_actor_seq():\n",
    "    # Initialize weights\n",
    "    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "    last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation=\"leaky_relu\", input_shape=(num_states,)),\n",
    "        layers.Dense(128, activation=\"leaky_relu\"),\n",
    "        layers.Dense(256, activation=\"leaky_relu\"),\n",
    "        layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        layers.Dense(num_actions, kernel_initializer=last_init),\n",
    "        # layers.Dense(num_actions, activation='softmax'),\n",
    "        # This is rescale the output from between -1 - 1 to the appropriate action space scale\n",
    "        layers.Lambda(lambda x: x * upper_bound)\n",
    "    ])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation=\"leaky_relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"leaky_relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions,))\n",
    "    action_out = layers.Dense(32, activation=\"leaky_relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    # model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "actor_weights_file = 'actor_weights'\n",
    "critic_weights_file = 'critic_weights'\n",
    "\n",
    "target_actor_weights_file = 'target_actor_weights'\n",
    "target_critic_weights_file = 'target_critic_weights'\n",
    "\n",
    "load_weights = False\n",
    "loat_target_weights = False\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "if load_weights:\n",
    "    actor_model.load_weights(actor_weights_file)\n",
    "    critic_model.load_weights(critic_weights_file)\n",
    "\n",
    "if loat_target_weights:\n",
    "    target_actor.load_weights(target_actor_weights_file)\n",
    "    target_critic.load_weights(target_critic_weights_file)\n",
    "else:\n",
    "    # Making the weights equal initially\n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.0015\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "\n",
    "# How many resets we're running for.\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "patience = gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "std_dev = 0.2\n",
    "# This used to be a custom OUActionNoise class, but it a paper came out saying that\n",
    "# gaussian noise works just as well\n",
    "noise = lambda std=std_dev: np.random.normal(scale=std, size=env.action_space.shape)\n",
    "add_noise = True\n",
    "noise_cooldown = actor_lr * 4\n",
    "\n",
    "reset_weights = True\n",
    "\n",
    "if not INIT_WEIGHTS:\n",
    "    actor_model.save_weights('actor_model_init.h5')\n",
    "    critic_model.save_weights('critic_model_init.h5')\n",
    "    target_actor.save_weights('target_actor_init.h5')\n",
    "    target_critic.save_weights('target_critic_init.h5')\n",
    "    INIT_WEIGHTS = True\n",
    "\n",
    "\n",
    "# ks.utils.plot_model(actor_model,   show_layer_activations=True, show_shapes=True)\n",
    "# ks.utils.plot_model(critic_model,  show_layer_activations=True, show_shapes=True) <-\n",
    "# ks.utils.plot_model(target_actor,  show_layer_activations=True, show_shapes=True)\n",
    "# ks.utils.plot_model(target_critic, show_layer_activations=True, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update called\n",
      "update called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 11:47:16.929839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7a018d67c060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-27 11:47:16.929866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-05-27 11:47:16.935791: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-27 11:47:16.954911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8800\n",
      "2024-05-27 11:47:17.022029: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Falsede * 0 * Avg Reward is ==> -10974.077947321271\n",
      "Falsede * 1 * Avg Reward is ==> -28826.01391245632\n",
      "Falsede * 2 * Avg Reward is ==> -22151.253519772163\n",
      "Falsede * 3 * Avg Reward is ==> -7074.230443095497\n",
      "Falsede * 4 * Avg Reward is ==> -3123.4426643243246\n",
      "Falsede * 5 * Avg Reward is ==> 3697.4594250831674\n",
      "Episode * 6 * Avg Reward is ==> 7317.18291846356\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeke/Software/miniconda3/envs/tf2/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "if reset_weights:\n",
    "    actor_model.load_weights('actor_model_init.h5')\n",
    "    critic_model.load_weights('critic_model_init.h5')\n",
    "    target_actor.load_weights('target_actor_init.h5')\n",
    "    target_critic.load_weights('target_critic_init.h5')\n",
    "\n",
    "# To store reward history of each episode (for plotting)\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes (for plotting)\n",
    "avg_reward_list = []\n",
    "\n",
    "try:\n",
    "    # Run through `total_episodes` number of enviorment resets\n",
    "    for ep in range(total_episodes):\n",
    "        prev_state, _ = env.reset()\n",
    "        # The env.search_space here (and after step()) is to normalize the values to within 0-1 so the NN can interpret them\n",
    "        # Note that this assumes the search_space is greater than pi/2 (which shouldn't be a problem)\n",
    "        prev_state = np.array([prev_state])/env.search_space\n",
    "        episodic_reward = 0\n",
    "        cnt = 0\n",
    "\n",
    "        # Run/step through a single episodes\n",
    "        while True:\n",
    "            cnt += 1\n",
    "            env.render()\n",
    "            # Slow it down so I can see it doing things\n",
    "            # time.sleep(.1)\n",
    "            # Show the enviorment (comment this out to run headless)\n",
    "            env.print['a'] = f'State: {prev_state}'\n",
    "\n",
    "            # This is the policy -- deciding what action to take\n",
    "            # Get the main actor output (i.e. \"which action do I think we should take?\")\n",
    "            sampled_actions = tf.squeeze(actor_model(prev_state)).numpy()\n",
    "            env.print['b'] = f'Action: {sampled_actions}'\n",
    "            if add_noise:\n",
    "                # This should make the noise fade out over time (proportional to the actor learning rate)\n",
    "                # We want to fade the noise over time, *and* as we step through specific episodes\n",
    "                total_cooldown   = max(std_dev - (ep  * noise_cooldown), 0)\n",
    "                episode_cooldown = max(std_dev - (cnt * noise_cooldown), 0)\n",
    "                sampled_actions += noise((total_cooldown + episode_cooldown) / 2)\n",
    "                # sampled_actions += noise(max(std_dev - (cnt * noise_cooldown), 0))\n",
    "                env.print['c'] = f'Action + noise: {sampled_actions}'\n",
    "\n",
    "            # Make sure action is the action space\n",
    "            action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            # Recieve state and reward from environment.\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            state = np.array([state])/env.search_space\n",
    "            env.print['d'] = f'Fresh State: {state}'\n",
    "\n",
    "            buffer.record(prev_state, action, reward, state)\n",
    "            episodic_reward += reward\n",
    "\n",
    "            # This is where the Bellman equation is implemented\n",
    "            buffer.learn()\n",
    "            update_target(target_actor.variables, actor_model.variables, tau)\n",
    "            update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "        # Episode is done, now do some calculations\n",
    "        # These are all just for plotting, not actually important to the algorithm\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "        # Mean of last 40 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward), end='\\r')\n",
    "\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Episodes versus Avg. Rewards graph\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(actor_weights_file)\n",
    "critic_model.save_weights(critic_weights_file)\n",
    "\n",
    "target_actor.save_weights(target_actor_weights_file)\n",
    "target_critic.save_weights(target_critic_weights_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
