{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# TODO: Add a thing that checks in on the enviorment to see if it isn't improving any longer and stops that epoch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras as ks\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "import SquarePacking.env\n",
    "# !pip install -e SquarePacking\n",
    "# from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space -> 15\n",
      "Size of Action Space -> 15\n",
      "Max Value of an Action -> [0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05 0.03 0.05 0.05\n",
      " 0.03]\n",
      "Min Value of an Action -> [-0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03 -0.05 -0.05 -0.03\n",
      " -0.05 -0.05 -0.03]\n"
     ]
    }
   ],
   "source": [
    "# Configuration & Enviorment\n",
    "\n",
    "# I don't need it to be determinisitic\n",
    "# seed = 42\n",
    "# tf.random.set_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# Set up the enviorment\n",
    "env = gym.make(\"SquarePacking/Square-v0\", N=5, search_space=10, render_mode=\"pygame\", shift_rate=.05, rot_rate=.03, max_steps=300, bound_method='loop', flatten=True, boundary=2, max_overlap=.1)\n",
    "\n",
    "num_states = np.product(env.observation_space.shape)\n",
    "print(\"Size of State Space -> {}\".format(num_states))\n",
    "num_actions = np.product(env.action_space.shape)\n",
    "print(\"Size of Action Space -> {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high\n",
    "lower_bound = env.action_space.low\n",
    "\n",
    "print(\"Max Value of an Action -> {}\".format(upper_bound))\n",
    "print(\"Min Value of an Action -> {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self, std=None):\n",
    "        if std is None: \n",
    "            std = self.std_dev\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + std * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, state, action, reward, next_state):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = state\n",
    "        self.action_buffer[index] = action\n",
    "        self.reward_buffer[index] = reward\n",
    "        self.next_state_buffer[index] = next_state\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch):\n",
    "        # Training and updating Actor & Critic networks. \n",
    "        with tf.GradientTape() as tape:\n",
    "            # What we think we should do\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            # How good we think that is (the target Q-Network)\n",
    "            target_evaluation = target_critic([next_state_batch, target_actions], training=True)\n",
    "            y = reward_batch + gamma * target_evaluation\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            # Get the average amount we were off by in predicting how well we would do, squared, and that's the loss\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "# This updates target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor & Critic Models\n",
    "# Tells a later cell to reset the weights (since we might have changed how we initialize them in this cell)\n",
    "INIT_WEIGHTS = False \n",
    "\n",
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    # last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n",
    "    # last_init = tf.random_uniform_initializer(minval=-3e-4, maxval=3e-4)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n",
    "    # outputs = layers.Dense(num_actions, activation='softsign', kernel_initializer=last_init)(out)\n",
    "    outputs = layers.Dense(num_actions, activation='softsign')(out)\n",
    "\n",
    "    # This is rescale the output from between -1 - 1 to the appropriate action space scale\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "# Exactly the same, but using Sequential, which I understand better\n",
    "def get_actor_seq():\n",
    "    # Initialize weights\n",
    "    last_init = tf.random_uniform_initializer(minval=-env.shift_rate, maxval=env.shift_rate)\n",
    "    # last_init = tf.random_uniform_initializer(minval=-3e-3, maxval=3e-3)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation=\"leaky_relu\", input_shape=(num_states,)),\n",
    "        layers.Dense(128, activation=\"leaky_relu\"),\n",
    "        layers.Dense(256, activation=\"leaky_relu\"),\n",
    "        layers.Dense(64, activation=\"leaky_relu\"),\n",
    "        layers.Dense(num_actions, kernel_initializer=last_init),\n",
    "        # layers.Dense(num_actions, activation='softmax'),\n",
    "        # This is rescale the output from between -1 - 1 to the appropriate action space scale\n",
    "        layers.Lambda(lambda x: x * upper_bound)\n",
    "    ])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states,))\n",
    "    state_out = layers.Dense(16, activation=\"leaky_relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"leaky_relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions,))\n",
    "    action_out = layers.Dense(32, activation=\"leaky_relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"leaky_relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    # model.summary()\n",
    "    return model\n",
    "    \n",
    "# Again, same thing but using Sequential instead\n",
    "def get_critic_seq():\n",
    "    state_model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation=\"leaky_relu\", input_shape=(num_states,)),\n",
    "        layers.Dense(256, activation=\"leaky_relu\")\n",
    "    ])\n",
    "\n",
    "    action_model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation=\"leaky_relu\", input_shape=(num_actions,)),\n",
    "        layers.Dense(256, activation=\"leaky_relu\")\n",
    "    ])\n",
    "\n",
    "    concat = layers.Concatenate()([state_model.output, action_model.output])\n",
    "    out = layers.Dense(400, activation=\"leaky_relu\")(concat)\n",
    "    out = layers.Dense(600, activation=\"leaky_relu\")(out)\n",
    "    outputs = layers.Dense(1, activation='softmax')(out) #, activation='tanh'\n",
    "\n",
    "    model = tf.keras.Model(inputs=[state_model.input, action_model.input], outputs=outputs)\n",
    "    # model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "actor_weights_file = 'actor_weights'\n",
    "critic_weights_file = 'critic_weights'\n",
    "\n",
    "target_actor_weights_file = 'target_actor_weights'\n",
    "target_critic_weights_file = 'target_critic_weights'\n",
    "\n",
    "load_weights = False\n",
    "loat_target_weights = False\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "if load_weights:\n",
    "    actor_model.load_weights(actor_weights_file)\n",
    "    critic_model.load_weights(critic_weights_file)\n",
    "\n",
    "if loat_target_weights:\n",
    "    target_actor.load_weights(target_actor_weights_file)\n",
    "    target_critic.load_weights(target_critic_weights_file)\n",
    "else:\n",
    "    # Making the weights equal initially\n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.0015\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "patience = gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "std_dev = 0.2 # was .2\n",
    "# noise = OUActionNoise(mean=np.zeros(1), std_deviation=std_dev * np.ones(1))\n",
    "noise = lambda std=std_dev: np.random.normal(scale=std, size=env.action_space.shape)\n",
    "add_noise = True\n",
    "noise_cooldown = actor_lr * 4\n",
    "\n",
    "reset_weights = True\n",
    "\n",
    "if not INIT_WEIGHTS:\n",
    "    actor_model.save_weights('actor_model_init.h5')\n",
    "    critic_model.save_weights('critic_model_init.h5')\n",
    "    target_actor.save_weights('target_actor_init.h5')\n",
    "    target_critic.save_weights('target_critic_init.h5')\n",
    "    INIT_WEIGHTS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> 51568.1049975111\n",
      "Episode * 1 * Avg Reward is ==> 122763.95649623811\n",
      "Episode * 2 * Avg Reward is ==> 100810.14639118737\n",
      "Episode * 3 * Avg Reward is ==> 128681.41137650133\n",
      "Episode * 4 * Avg Reward is ==> 149695.45642299173\n",
      "Episode * 5 * Avg Reward is ==> 161387.27910446527\n",
      "Episode * 6 * Avg Reward is ==> 172883.080142939\n",
      "Episode * 7 * Avg Reward is ==> 184980.44190821348\n",
      "Episode * 8 * Avg Reward is ==> 191471.6229343308\n",
      "Episode * 9 * Avg Reward is ==> 193209.78950266022\n",
      "Episode * 10 * Avg Reward is ==> 192194.67224670344\n",
      "Episode * 11 * Avg Reward is ==> 186996.39442286536\n",
      "Episode * 12 * Avg Reward is ==> 184265.0690208532\n",
      "Episode * 13 * Avg Reward is ==> 182913.75033782233\n",
      "Episode * 14 * Avg Reward is ==> 184845.26297492327\n",
      "Episode * 15 * Avg Reward is ==> 189448.4160353851\n",
      "Episode * 16 * Avg Reward is ==> 188981.56223495599\n",
      "Episode * 17 * Avg Reward is ==> 189373.2811117411\n",
      "Episode * 18 * Avg Reward is ==> 190958.88297679622\n",
      "Episode * 19 * Avg Reward is ==> 190945.63129608275\n",
      "Episode * 20 * Avg Reward is ==> 188900.31794219403\n",
      "Episode * 21 * Avg Reward is ==> 187726.14660644624\n",
      "Episode * 22 * Avg Reward is ==> 189856.90421739456\n",
      "Episode * 23 * Avg Reward is ==> 187336.3881854069\n",
      "Episode * 24 * Avg Reward is ==> 189294.52874211257\n",
      "Episode * 25 * Avg Reward is ==> 191225.22402459977\n",
      "Episode * 26 * Avg Reward is ==> 192276.1782082764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(sampled_actions, lower_bound, upper_bound)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Recieve state and reward from environment.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([state])\u001b[39m/\u001b[39menv\u001b[39m.\u001b[39msearch_space\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePackingV2.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m env\u001b[39m.\u001b[39mprint(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFresh State: \u001b[39m\u001b[39m{\u001b[39;00mstate\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:51\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     41\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:38\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/hello/python/AI/SquarePacking/SquarePacking/SquarePacking/env/SquareEnv.py:229\u001b[0m, in \u001b[0;36mSquareEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnknown `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbound_method\u001b[39m}\u001b[39;00m\u001b[39m` bound_method provided\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msquares \u001b[39m=\u001b[39m space2MultiPolygon(newObs)\n\u001b[0;32m--> 229\u001b[0m info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_info()\n\u001b[1;32m    230\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_terminated()\n\u001b[1;32m    231\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlossFunc()\n",
      "File \u001b[0;32m~/hello/python/AI/SquarePacking/SquarePacking/SquarePacking/env/SquareEnv.py:145\u001b[0m, in \u001b[0;36mSquareEnv._get_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_info\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    143\u001b[0m         \u001b[39m# 'Overlaps': not self.squares.is_valid,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moverlap\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverlap_area(),\n\u001b[0;32m--> 145\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlen\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mside_len(),\n\u001b[1;32m    146\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mwasted\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwasted_space(),\n\u001b[1;32m    147\u001b[0m         \u001b[39m# 'loss': lossFunc(self.squares),\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     }\n",
      "File \u001b[0;32m~/hello/python/AI/SquarePacking/SquarePacking/SquarePacking/env/SquareEnv.py:368\u001b[0m, in \u001b[0;36mSquareEnv.side_len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mside_len\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 368\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msquares\u001b[39m.\u001b[39;49mminimum_rotated_rectangle\u001b[39m.\u001b[39mexterior\u001b[39m.\u001b[39mcoords\u001b[39m.\u001b[39mxy\n\u001b[1;32m    369\u001b[0m     edge_length \u001b[39m=\u001b[39m (Point(x[\u001b[39m0\u001b[39m], y[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mdistance(Point(x[\u001b[39m1\u001b[39m], y[\u001b[39m1\u001b[39m])), Point(x[\u001b[39m1\u001b[39m], y[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mdistance(Point(x[\u001b[39m2\u001b[39m], y[\u001b[39m2\u001b[39m])))\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(edge_length)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shapely/geometry/base.py:411\u001b[0m, in \u001b[0;36mBaseGeometry.minimum_rotated_rectangle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimum_rotated_rectangle\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    401\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m    Returns the oriented envelope (minimum rotated rectangle) that\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m    encloses the geometry.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39m    Alias of `oriented_envelope`.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mreturn\u001b[39;00m shapely\u001b[39m.\u001b[39;49moriented_envelope(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shapely/decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m array_args:\n\u001b[1;32m     76\u001b[0m         arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m arr, old_flag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/shapely/constructive.py:1000\u001b[0m, in \u001b[0;36moriented_envelope\u001b[0;34m(geometry, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39m@requires_geos\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m3.6.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m \u001b[39m@multithreading_enabled\u001b[39m\n\u001b[1;32m    968\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moriented_envelope\u001b[39m(geometry, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    969\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[39m    Computes the oriented envelope (minimum rotated rectangle)\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m    that encloses an input geometry.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39m    <POLYGON EMPTY>\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49moriented_envelope(geometry, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "if reset_weights:\n",
    "    actor_model.load_weights('actor_model_init.h5')\n",
    "    critic_model.load_weights('critic_model_init.h5')\n",
    "    target_actor.load_weights('target_actor_init.h5')\n",
    "    target_critic.load_weights('target_critic_init.h5')\n",
    "\n",
    "# To store reward history of each episode (for plotting)\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes (for plotting)\n",
    "avg_reward_list = []\n",
    "\n",
    "try:\n",
    "    # Run through `total_episodes` number of enviorment resets\n",
    "    for ep in range(total_episodes):\n",
    "        prev_state, _ = env.reset()\n",
    "        # The /env.search_space here (and after step()) is to normalize the values to within 0-1 so the NN can interpret them\n",
    "        # Note that this assumes the search_space is greater than pi/2 (which shouldn't be a problem)\n",
    "        prev_state = np.array([prev_state])/env.search_space\n",
    "        episodic_reward = 0\n",
    "        cnt = 0\n",
    "\n",
    "        # Run/step through a single episodes\n",
    "        while True:\n",
    "            cnt += 1\n",
    "            # Slow it down so I can see it doing things\n",
    "            # time.sleep(.1)\n",
    "            # Show the enviorment (comment this out to run headless)\n",
    "            env.render()\n",
    "            env.print(f'State: {state}')\n",
    "\n",
    "            # This is the policy -- deciding what action to take\n",
    "            # Get the main actor output (i.e. \"which action do I think we should take?\")\n",
    "            sampled_actions = tf.squeeze(actor_model(prev_state)).numpy()\n",
    "            env.print(f'Action: {sampled_actions}')\n",
    "            if add_noise:\n",
    "                # This should make the noise fade out over time (proportional to the actor learning rate)\n",
    "                # We want to fade the noise over time, *and* as we step through specific episodes\n",
    "                total_cooldown   = max(std_dev - (ep  * noise_cooldown), 0)\n",
    "                episode_cooldown = max(std_dev - (cnt * noise_cooldown), 0)\n",
    "                sampled_actions += noise((total_cooldown + episode_cooldown) / 2)\n",
    "                # sampled_actions += noise(max(std_dev - (cnt * noise_cooldown), 0))\n",
    "                env.print(f'Action + noise: {sampled_actions}')\n",
    "                \n",
    "            # Make sure action is the action space \n",
    "            action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            # Recieve state and reward from environment.\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            state = np.array([state])/env.search_space\n",
    "            env.print(f'Fresh State: {state}')\n",
    "\n",
    "            buffer.record(prev_state, action, reward, state)\n",
    "            episodic_reward += reward\n",
    "\n",
    "            # This is where the Bellman equation is implemented\n",
    "            buffer.learn()\n",
    "            update_target(target_actor.variables, actor_model.variables, tau)\n",
    "            update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "        # Episode is done, now do some calculations\n",
    "        # These are all just for plotting, not actually important to the algorithm\n",
    "        ep_reward_list.append(episodic_reward)\n",
    "        # Mean of last 40 episodes\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    \n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Episodes versus Avg. Rewards graph \n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(actor_weights_file)\n",
    "critic_model.save_weights(critic_weights_file)\n",
    "\n",
    "target_actor.save_weights(target_actor_weights_file)\n",
    "target_critic.save_weights(target_critic_weights_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
