{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/leonard/hello/python/AI/SquarePacking/SquarePacking\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gymnasium in /home/leonard/.local/lib/python3.10/site-packages (from SquarePacking==0.0.1) (0.27.1)\n",
      "Requirement already satisfied: matplotlib in /home/leonard/.local/lib/python3.10/site-packages (from SquarePacking==0.0.1) (3.5.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium->SquarePacking==0.0.1) (4.5.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium->SquarePacking==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium->SquarePacking==0.0.1) (1.21.6)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium->SquarePacking==0.0.1) (0.0.1)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from gymnasium->SquarePacking==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (1.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (4.31.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/leonard/.local/lib/python3.10/site-packages (from matplotlib->SquarePacking==0.0.1) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->SquarePacking==0.0.1) (1.16.0)\n",
      "Installing collected packages: SquarePacking\n",
      "  Attempting uninstall: SquarePacking\n",
      "    Found existing installation: SquarePacking 0.0.1\n",
      "    Uninstalling SquarePacking-0.0.1:\n",
      "      Successfully uninstalled SquarePacking-0.0.1\n",
      "  Running setup.py develop for SquarePacking\n",
      "Successfully installed SquarePacking\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "# from gym.wrappers import FlattenObservation\n",
    "# from gym.utils.env_checker import check_env\n",
    "# import tabletop\n",
    "# import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "# import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "# from gym.utils.play import play\n",
    "# from Cope import debug\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import SquarePacking.env\n",
    "!pip install -e SquarePacking\n",
    "from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Model from Online\n",
    "\n",
    "# The inputs are in the obvervation space\n",
    "num_inputs = 3*N\n",
    "# We want to nudge each box a little in each parameter\n",
    "num_outputs = num_inputs\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "action = layers.Dense(num_outputs, activation=\"tanh\")(common)\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:  # Run until solved\n",
    "    # obs, info = env.reset(seed=seed)\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            if GUI:\n",
    "                env.render()\n",
    "\n",
    "            # state = tf.convert_to_tensor(np.reshape(np.array((state['ant'], state['nearest food'], state['has food'])), (3,)))\n",
    "            state = tf.convert_to_tensor(obs)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            action_probs, critic_value = model(state)\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # Sample action from action probability distribution\n",
    "            # I don't think this works, because \n",
    "            # action = np.random.choice(num_outputs, p=np.squeeze(action_probs))\n",
    "            # action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "            action_probs_history.append(tf.math.log(action_probs))\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done, info = env.step(action_probs)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + patience * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(f\"running reward: {running_reward:.2f} at episode {episode_count}\")\n",
    "\n",
    "    # if running_reward > 195:  # Condition to consider the task solved\n",
    "    if info['solved']:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:129: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/leonard/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:35: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (4, 3)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configuration & Enviorment\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set up the enviorment\n",
    "env      = gym.make(\"SquarePacking/Square-v0\", N=4, render_mode=\"shapely\", shift_rate=.05, rot_rate=.05)\n",
    "test_env = gym.make(\"SquarePacking/Square-v0\", N=4, render_mode=\"shapely\", shift_rate=.05, rot_rate=.05)\n",
    "\n",
    "# Discount factor for past rewards. (Always between 0 and 1.)\n",
    "patience = gamma = 0.99\n",
    "# Number of epochs to run and train agent.\n",
    "epochs = 100\n",
    "# Number of steps of interaction (state-action pairs) for the agent and the environment in each epoch.\n",
    "steps_per_epoch = 4000\n",
    "# Maximum length of replay buffer.\n",
    "replay_size = int(1e6)\n",
    "# Interpolation factor in polyak averaging for target networks. Target networks are updated towards main networks according to:\n",
    "# θ_target <- pθ_target + (1-p)θ\n",
    "# where p is polyak. (Always between 0 and 1, usually close to 1.)\n",
    "polyak = 0.995\n",
    "# Learning rate for policy.\n",
    "pi_lr = policy_lr = 1e-3\n",
    "# Learning rate for Q-networks.\n",
    "q_lr = Q_lr = 1e-3\n",
    "# Minibatch size for SGD.\n",
    "batch_size = 100\n",
    "# Number of steps for uniform-random action selection, before running real policy. Helps exploration.\n",
    "start_steps = 10000\n",
    "# Number of env interactions to collect before starting to do gradient descent updates. Ensures replay buffer is full enough for useful updates.\n",
    "update_after = 1000\n",
    "# Number of env interactions that should elapse between gradient descent updates. Note: Regardless of how long\n",
    "# you wait between updates, the ratio of env steps to gradient steps is locked to 1.\n",
    "update_every = 50\n",
    "# Stddev for Gaussian exploration noise added to policy at training time. (At test time, no noise is added.)\n",
    "act_noise = 0.1\n",
    "# Number of episodes to test the deterministic policy at the end of each epoch.\n",
    "num_test_episodes = 10\n",
    "# Maximum length of trajectory / episode / rollout.\n",
    "max_ep_len = 1000\n",
    "# The space dimentions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "act_limit = env.action_space.high[0]\n",
    "# Inputs to computation graph\n",
    "# x_ph, a_ph, x2_ph, r_ph, d_ph = [tf.placeholder(dtype=tf.float32, shape=(None,dim) if dim else (None,)) for dim in (obs_dim, act_dim, obs_dim, None, None)]\n",
    "# x_ph, a_ph, x2_ph, r_ph, d_ph = obs_dim, act_dim, obs_dim, None, None\n",
    "# Experience buffer\n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "# How often (in terms of gap between epochs) to save the current policy and value function.\n",
    "# save_freq = 1\n",
    "# Any kwargs appropriate for the actor_critic function you provided to DDPG.\n",
    "# ac_kwargs = dict(hidden_sizes=[args.hid]*args.l)\n",
    "# Share information about action space with policy architecture\n",
    "# ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "\n",
    "#? Don't know what this means\n",
    "# eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ac_kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb Cell 5\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Main outputs from computation graph\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# with tf.variable_scope('main'):\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pi, q, q_pi \u001b[39m=\u001b[39m actor_critic(x_ph, a_ph, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mac_kwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Target networks\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# with tf.variable_scope('target'):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Note that the action placeholder going to actor_critic here is irrelevant, because we only need q_targ(s, pi_targ(s)).\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/leonard/hello/python/AI/SquarePacking/RLSquarePacking.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pi_targ, _, q_pi_targ  \u001b[39m=\u001b[39m actor_critic(x2_ph, a_ph, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mac_kwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ac_kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "# Main outputs from computation graph\n",
    "\n",
    "# Actor-critic functions\n",
    "\n",
    "def actor_critic():\n",
    "    \"\"\" A function which takes in placeholder symbols for state, ``x_ph``, and action, ``a_ph``, \n",
    "    and returns the main outputs from the agent's Tensorflow computation graph:\n",
    "    ===========  ================  ======================================\n",
    "    Symbol       Shape             Description\n",
    "    ===========  ================  ======================================\n",
    "    ``pi``       (batch, act_dim)  | Deterministically computes actions\n",
    "                                   | from policy given states.\n",
    "    ``q``        (batch,)          | Gives the current estimate of Q* for\n",
    "                                   | states in ``x_ph`` and actions in\n",
    "                                   | ``a_ph``.\n",
    "    ``q_pi``     (batch,)          | Gives the composition of ``q`` and\n",
    "                                   | ``pi`` for states in ``x_ph``:\n",
    "                                   | q(x, pi(x)).\n",
    "    ===========  ================  ======================================\n",
    "    \"\"\"\n",
    "\n",
    "    def mlp(size, hidden_sizes=(256, 256)):\n",
    "        for h in hidden_sizes[:-1]:\n",
    "            unknownVar = tf.layers.Dense(size, units=h, activation='relu')\n",
    "        return tf.layers.Dense(unknownVar, units=hidden_sizes[-1], activation='tanh')\n",
    "\n",
    "    _act_dim = act_dim.shape.as_list()[-1]\n",
    "    _act_limit = env.action_space.high[0]\n",
    "\n",
    "    pi = act_limit * mlp(list(hidden_sizes)+[act_dim])\n",
    "    q = tf.squeeze(mlp(tf.concat([obs_dim, act_dim], axis=-1), list(hidden_sizes)+[1], tf.nn.relu, None), axis=1)\n",
    "    q_pi = tf.squeeze(mlp(tf.concat([obs_dim, pi], axis=-1), list(hidden_sizes)+[1], tf.nn.relu, None), axis=1)\n",
    "\n",
    "    return pi, q, q_pi\n",
    "\n",
    "\n",
    "# with tf.variable_scope('main'):\n",
    "pi, q, q_pi = actor_critic(obs_dim, act_dim, **ac_kwargs)\n",
    "\n",
    "# Target networks\n",
    "# with tf.variable_scope('target'):\n",
    "# Note that the action placeholder going to actor_critic here is irrelevant, because we only need q_targ(s, pi_targ(s)).\n",
    "# ac_kwargs = dict(hidden_sizes=[args.hid]*args.l)\n",
    "# ac_kwargs['action_space'] = env.action_space\n",
    "pi_targ, _, q_pi_targ  = actor_critic(obs_dim, act_dim, hidden_sizes=[args.hid]*args.l, action_space=)\n",
    "\n",
    "# Count variables\n",
    "# var_counts = tuple(core.count_vars(scope) for scope in ['main/pi', 'main/q', 'main'])\n",
    "# print('\\nNumber of parameters: \\t pi: %d, \\t q: %d, \\t total: %d\\n'%var_counts)\n",
    "\n",
    "\n",
    "# Bellman backup for Q function\n",
    "# backup = tf.stop_gradient(r_ph + patience*(1-d_ph)*q_pi_targ)\n",
    "backup = tf.stop_gradient(r_ph + patience*(1-int(done))*q_pi_targ)\n",
    "\n",
    "# DDPG losses\n",
    "pi_loss = -tf.reduce_mean(q_pi)\n",
    "q_loss = tf.reduce_mean((q-backup)**2)\n",
    "\n",
    "# Separate train ops for pi, q\n",
    "pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
    "q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
    "train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
    "train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
    "\n",
    "# Polyak averaging for target variables\n",
    "target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                            for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "# Initializing targets to match main variables\n",
    "target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                            for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(target_init)\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "start_time = time.time()\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_action(obs, noise_scale):\n",
    "    action = sess.run(pi, feed_dict={x_ph: obs.reshape(1,-1)})[0]\n",
    "    action += noise_scale * np.random.randn(act_dim)\n",
    "    return np.clip(action, -act_limit, act_limit)\n",
    "\n",
    "def test_agent():\n",
    "    for j in range(num_test_episodes):\n",
    "        obs, done, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "        while not(done or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            obs, reward, done, _ = test_env.step(get_action(obs, 0))\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "        # logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for t in range(total_steps):\n",
    "    # Until start_steps have elapsed, randomly sample actions\n",
    "    # from a uniform distribution for better exploration. Afterwards,\n",
    "    # use the learned policy (with some noise, via act_noise).\n",
    "    if t > start_steps:\n",
    "        action = get_action(o, act_noise)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "    # Step the env\n",
    "    obs2, reward, done, _ = env.step(a)\n",
    "    ep_ret += reward\n",
    "    ep_len += 1\n",
    "\n",
    "    # Ignore the \"done\" signal if it comes from hitting the time\n",
    "    # horizon (that is, when it's an artificial terminal signal\n",
    "    # that isn't based on the agent's state)\n",
    "    done = False if ep_len == max_ep_len else done\n",
    "\n",
    "    # Store experience to replay buffer\n",
    "    replay_buffer.store(obs, action, reward, obs2, done)\n",
    "\n",
    "    # Super critical, easy to overlook step: make sure to update\n",
    "    # most recent observation!\n",
    "    obs = obs2\n",
    "\n",
    "    # End of trajectory handling\n",
    "    if done or (ep_len == max_ep_len):\n",
    "        # logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "        obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update handling\n",
    "    if t >= update_after and t % update_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            feed_dict = {x_ph: batch['obs1'],\n",
    "                            x2_ph: batch['obs2'],\n",
    "                            a_ph: batch['acts'],\n",
    "                            r_ph: batch['rews'],\n",
    "                            d_ph: batch['done']\n",
    "                        }\n",
    "\n",
    "            # Q-learning update\n",
    "            outs = sess.run([q_loss, q, train_q_op], feed_dict)\n",
    "            # logger.store(LossQ=outs[0], QVals=outs[1])\n",
    "\n",
    "            # Policy update\n",
    "            outs = sess.run([pi_loss, train_pi_op, target_update], feed_dict)\n",
    "            # logger.store(LossPi=outs[0])\n",
    "\n",
    "    # End of epoch wrap-up\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "\n",
    "        # Save model\n",
    "        # if (epoch % save_freq == 0) or (epoch == epochs):\n",
    "        #     logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        test_agent()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
